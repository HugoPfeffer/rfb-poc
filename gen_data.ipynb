{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNi+CB4OnR39q5c9r20uDma",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HugoPfeffer/rfb-poc/blob/main/gen_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sdv\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HldTrl1KJF32",
        "outputId": "6d1477fd-a2a9-46cb-be1b-9b837c628037"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sdv in /usr/local/lib/python3.10/dist-packages (1.17.3)\n",
            "Requirement already satisfied: boto3<2.0.0,>=1.28 in /usr/local/lib/python3.10/dist-packages (from sdv) (1.35.92)\n",
            "Requirement already satisfied: botocore<2.0.0,>=1.31 in /usr/local/lib/python3.10/dist-packages (from sdv) (1.35.92)\n",
            "Requirement already satisfied: cloudpickle>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from sdv) (3.1.0)\n",
            "Requirement already satisfied: graphviz>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from sdv) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.23.3 in /usr/local/lib/python3.10/dist-packages (from sdv) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from sdv) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.29 in /usr/local/lib/python3.10/dist-packages (from sdv) (4.67.1)\n",
            "Requirement already satisfied: copulas>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from sdv) (0.12.0)\n",
            "Requirement already satisfied: ctgan>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from sdv) (0.10.2)\n",
            "Requirement already satisfied: deepecho>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from sdv) (0.6.1)\n",
            "Requirement already satisfied: rdt>=1.13.2.dev0 in /usr/local/lib/python3.10/dist-packages (from sdv) (1.13.2)\n",
            "Requirement already satisfied: sdmetrics>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from sdv) (0.18.0)\n",
            "Requirement already satisfied: platformdirs>=4.0 in /usr/local/lib/python3.10/dist-packages (from sdv) (4.3.6)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from sdv) (6.0.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0.0,>=1.28->sdv) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0.0,>=1.28->sdv) (0.10.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<2.0.0,>=1.31->sdv) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<2.0.0,>=1.31->sdv) (2.2.3)\n",
            "Requirement already satisfied: plotly>=5.10.0 in /usr/local/lib/python3.10/dist-packages (from copulas>=0.12.0->sdv) (5.24.1)\n",
            "Requirement already satisfied: scipy>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from copulas>=0.12.0->sdv) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from ctgan>=0.10.2->sdv) (2.5.1+cu121)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->sdv) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->sdv) (2024.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from rdt>=1.13.2.dev0->sdv) (1.6.0)\n",
            "Requirement already satisfied: Faker>=17 in /usr/local/lib/python3.10/dist-packages (from rdt>=1.13.2.dev0->sdv) (33.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from Faker>=17->rdt>=1.13.2.dev0->sdv) (4.12.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.10.0->copulas>=0.12.0->sdv) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=5.10.0->copulas>=0.12.0->sdv) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.31->sdv) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->rdt>=1.13.2.dev0->sdv) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->rdt>=1.13.2.dev0->sdv) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.2->sdv) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.2->sdv) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.2->sdv) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.2->sdv) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->ctgan>=0.10.2->sdv) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->ctgan>=0.10.2->sdv) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->ctgan>=0.10.2->sdv) (3.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "65fRWks1NhZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Cqq43gERNgGq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SoEAXgOfIGf3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sdv.single_table import GaussianCopulaSynthesizer\n",
        "from sdv.metadata import SingleTableMetadata\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of samples to generate\n",
        "NUM_SAMPLES = 1000\n",
        "FRAUD_PERCENTAGE = 0.05  # 5% of data will be fraudulent\n",
        "OUTLIER_PERCENTAGE = 0.02  # 2% of data will be outliers"
      ],
      "metadata": {
        "id": "lvBwcXfIIJdl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define possible categorical values\n",
        "FILING_STATUSES = ['Single', 'Married Filing Jointly', 'Married Filing Separately', 'Head of Household']\n",
        "STATES = [\n",
        "    'AC', 'AL', 'AP', 'AM', 'BA', 'CE', 'DF', 'ES', 'GO', 'MA',\n",
        "    'MT', 'MS', 'MG', 'PA', 'PB', 'PR', 'PE', 'PI', 'RJ', 'RN',\n",
        "    'RS', 'RO', 'RR', 'SC', 'SP', 'SE', 'TO'\n",
        "]"
      ],
      "metadata": {
        "id": "O0-hTpnKILM8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a dictionary for state names (optional, for reference)\n",
        "STATE_NAMES = {\n",
        "    'AC': 'Acre',\n",
        "    'AL': 'Alagoas',\n",
        "    'AP': 'Amapá',\n",
        "    'AM': 'Amazonas',\n",
        "    'BA': 'Bahia',\n",
        "    'CE': 'Ceará',\n",
        "    'DF': 'Distrito Federal',\n",
        "    'ES': 'Espírito Santo',\n",
        "    'GO': 'Goiás',\n",
        "    'MA': 'Maranhão',\n",
        "    'MT': 'Mato Grosso',\n",
        "    'MS': 'Mato Grosso do Sul',\n",
        "    'MG': 'Minas Gerais',\n",
        "    'PA': 'Pará',\n",
        "    'PB': 'Paraíba',\n",
        "    'PR': 'Paraná',\n",
        "    'PE': 'Pernambuco',\n",
        "    'PI': 'Piauí',\n",
        "    'RJ': 'Rio de Janeiro',\n",
        "    'RN': 'Rio Grande do Norte',\n",
        "    'RS': 'Rio Grande do Sul',\n",
        "    'RO': 'Rondônia',\n",
        "    'RR': 'Roraima',\n",
        "    'SC': 'Santa Catarina',\n",
        "    'SP': 'São Paulo',\n",
        "    'SE': 'Sergipe',\n",
        "    'TO': 'Tocantins'\n",
        "}\n"
      ],
      "metadata": {
        "id": "OW7zMuaJINuS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inject_outliers(data, percentage=OUTLIER_PERCENTAGE):\n",
        "    \"\"\"Inject outliers into the dataset.\"\"\"\n",
        "    num_outliers = int(len(data) * percentage)\n",
        "    outlier_indices = np.random.choice(len(data), num_outliers, replace=False)\n",
        "\n",
        "    outlier_data = data.copy()\n",
        "\n",
        "    for idx in outlier_indices:\n",
        "        # Randomly choose which feature to modify\n",
        "        feature = np.random.choice(['income', 'deductions', 'tax_paid', 'refund_claimed'])\n",
        "\n",
        "        if feature == 'income':\n",
        "            # Extremely high income\n",
        "            outlier_data.loc[idx, 'income'] *= np.random.uniform(10, 20)\n",
        "        elif feature == 'deductions':\n",
        "            # Unusually high deductions compared to income\n",
        "            outlier_data.loc[idx, 'deductions'] = outlier_data.loc[idx, 'income'] * np.random.uniform(0.8, 0.95)\n",
        "        elif feature == 'tax_paid':\n",
        "            # Suspiciously low tax paid\n",
        "            outlier_data.loc[idx, 'tax_paid'] *= np.random.uniform(0.1, 0.2)\n",
        "        elif feature == 'refund_claimed':\n",
        "            # Extremely high refund claims\n",
        "            outlier_data.loc[idx, 'refund_claimed'] = outlier_data.loc[idx, 'tax_paid'] * np.random.uniform(1.5, 2.0)\n",
        "\n",
        "    return outlier_data\n"
      ],
      "metadata": {
        "id": "UBAevX0ZIOhe"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fraudulent_patterns(data, percentage=FRAUD_PERCENTAGE):\n",
        "    \"\"\"Generate fraudulent patterns in the dataset.\"\"\"\n",
        "    num_fraud = int(len(data) * percentage)\n",
        "    fraud_indices = np.random.choice(len(data), num_fraud, replace=False)\n",
        "\n",
        "    fraud_data = data.copy()\n",
        "    fraud_data['is_fraudulent'] = 0  # Initialize fraud indicator column\n",
        "\n",
        "    for idx in fraud_indices:\n",
        "        fraud_type = np.random.choice([\n",
        "            'income_underreporting',\n",
        "            'deduction_inflation',\n",
        "            'refund_manipulation',\n",
        "            'compliance_manipulation'\n",
        "        ])\n",
        "\n",
        "        if fraud_type == 'income_underreporting':\n",
        "            # Underreport income but maintain high deductions\n",
        "            fraud_data.loc[idx, 'income'] *= np.random.uniform(0.4, 0.6)\n",
        "            fraud_data.loc[idx, 'compliance_score'] *= np.random.uniform(0.5, 0.7)\n",
        "\n",
        "        elif fraud_type == 'deduction_inflation':\n",
        "            # Inflate deductions relative to income\n",
        "            fraud_data.loc[idx, 'deductions'] = fraud_data.loc[idx, 'income'] * np.random.uniform(0.7, 0.9)\n",
        "            fraud_data.loc[idx, 'compliance_score'] *= np.random.uniform(0.6, 0.8)\n",
        "\n",
        "        elif fraud_type == 'refund_manipulation':\n",
        "            # Claim excessive refunds\n",
        "            fraud_data.loc[idx, 'refund_claimed'] = fraud_data.loc[idx, 'tax_paid'] * np.random.uniform(1.2, 1.8)\n",
        "            fraud_data.loc[idx, 'compliance_score'] *= np.random.uniform(0.4, 0.6)\n",
        "\n",
        "        elif fraud_type == 'compliance_manipulation':\n",
        "            # Multiple suspicious patterns\n",
        "            fraud_data.loc[idx, 'income'] *= np.random.uniform(0.6, 0.8)\n",
        "            fraud_data.loc[idx, 'deductions'] = fraud_data.loc[idx, 'income'] * np.random.uniform(0.6, 0.8)\n",
        "            fraud_data.loc[idx, 'compliance_score'] *= np.random.uniform(0.3, 0.5)\n",
        "\n",
        "        fraud_data.loc[idx, 'is_fraudulent'] = 1\n",
        "\n",
        "    return fraud_data\n"
      ],
      "metadata": {
        "id": "Wzegn_WIISgu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_base_data():\n",
        "    \"\"\"Generate initial synthetic tax data.\"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    data = {\n",
        "        'income': np.random.lognormal(11, 0.7, NUM_SAMPLES),\n",
        "        'filing_status': np.random.choice(FILING_STATUSES, NUM_SAMPLES),\n",
        "        'state': np.random.choice(STATES, NUM_SAMPLES),\n",
        "        'compliance_score': np.random.beta(8, 2, NUM_SAMPLES) * 100,\n",
        "    }\n",
        "\n",
        "    # Calculate dependent variables with more realistic patterns\n",
        "    data['deductions'] = np.where(\n",
        "        data['income'] > np.median(data['income']),\n",
        "        data['income'] * np.random.beta(2, 5, NUM_SAMPLES),  # Higher income brackets\n",
        "        data['income'] * np.random.beta(1.5, 6, NUM_SAMPLES)  # Lower income brackets\n",
        "    )\n",
        "\n",
        "    data['tax_paid'] = np.where(\n",
        "        data['income'] > np.median(data['income']),\n",
        "        data['income'] * np.random.beta(3, 7, NUM_SAMPLES),  # Higher income brackets\n",
        "        data['income'] * np.random.beta(2, 8, NUM_SAMPLES)  # Lower income brackets\n",
        "    )\n",
        "\n",
        "    data['refund_claimed'] = np.where(\n",
        "        np.random.random(NUM_SAMPLES) < 0.7,\n",
        "        data['tax_paid'] * np.random.beta(2, 5, NUM_SAMPLES),\n",
        "        0\n",
        "    )\n",
        "\n",
        "    return pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "s_p87m6SIVL8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_synthesizer(data):\n",
        "    \"\"\"Train the SDV synthesizer on the data.\"\"\"\n",
        "    metadata = SingleTableMetadata()\n",
        "    metadata.detect_from_dataframe(data)\n",
        "\n",
        "    # Update metadata for specific columns\n",
        "    metadata.update_column(\n",
        "        column_name='filing_status',\n",
        "        sdtype='categorical'\n",
        "    )\n",
        "    metadata.update_column(\n",
        "        column_name='state',\n",
        "        sdtype='categorical'\n",
        "    )\n",
        "\n",
        "    synthesizer = GaussianCopulaSynthesizer(metadata)\n",
        "    synthesizer.fit(data)\n",
        "\n",
        "    return synthesizer\n"
      ],
      "metadata": {
        "id": "JyBQ3hPnIXQx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_and_split_data(synthetic_data, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Normalize the numerical features and split the data into training and test sets.\n",
        "    \"\"\"\n",
        "    # Separate numerical and categorical columns\n",
        "    numerical_cols = ['income', 'deductions', 'tax_paid', 'refund_claimed', 'compliance_score']\n",
        "    categorical_cols = ['filing_status', 'state']\n",
        "\n",
        "    # Initialize the scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Normalize numerical columns\n",
        "    normalized_data = synthetic_data.copy()\n",
        "    normalized_data[numerical_cols] = scaler.fit_transform(synthetic_data[numerical_cols])\n",
        "\n",
        "    # Split the data\n",
        "    train_data, test_data = train_test_split(\n",
        "        normalized_data,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Save the scaler for future use\n",
        "    joblib.dump(scaler, 'tax_data_scaler.joblib')\n",
        "\n",
        "    return train_data, test_data, scaler"
      ],
      "metadata": {
        "id": "aGDuunqOIZzW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_data(num_samples=1000):\n",
        "    \"\"\"Generate and save synthetic tax data.\"\"\"\n",
        "    # Generate initial data\n",
        "    base_data = generate_base_data()\n",
        "\n",
        "    # Train synthesizer\n",
        "    synthesizer = train_synthesizer(base_data)\n",
        "\n",
        "    # Generate synthetic data\n",
        "    synthetic_data = synthesizer.sample(num_samples)\n",
        "\n",
        "    # Post-process the data\n",
        "    synthetic_data['income'] = synthetic_data['income'].round(2)\n",
        "    synthetic_data['deductions'] = synthetic_data['deductions'].round(2)\n",
        "    synthetic_data['tax_paid'] = synthetic_data['tax_paid'].round(2)\n",
        "    synthetic_data['refund_claimed'] = synthetic_data['refund_claimed'].round(2)\n",
        "    synthetic_data['compliance_score'] = synthetic_data['compliance_score'].round(2)\n",
        "\n",
        "    # Inject fraudulent patterns\n",
        "    synthetic_data = generate_fraudulent_patterns(synthetic_data)\n",
        "\n",
        "    # Inject outliers\n",
        "    synthetic_data = inject_outliers(synthetic_data)\n",
        "\n",
        "    # Save raw data to CSV\n",
        "    synthetic_data.to_csv('synthetic_tax_data.csv', index=False)\n",
        "\n",
        "    # Normalize and split the data\n",
        "    train_data, test_data, scaler = normalize_and_split_data(synthetic_data)\n",
        "\n",
        "    # Save train and test sets\n",
        "    train_data.to_csv('train_data.csv', index=False)\n",
        "    test_data.to_csv('test_data.csv', index=False)\n",
        "\n",
        "    return synthetic_data, train_data, test_data, scaler\n"
      ],
      "metadata": {
        "id": "U9oXfybQTog-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    synthetic_data, train_data, test_data, scaler = generate_synthetic_data()\n",
        "    print(\"Synthetic data generated and saved to 'synthetic_tax_data.csv'\")\n",
        "    print(f\"Training data saved to 'train_data.csv' (shape: {train_data.shape})\")\n",
        "    print(f\"Test data saved to 'test_data.csv' (shape: {test_data.shape})\")\n",
        "\n",
        "    # Print fraud statistics\n",
        "    fraud_count = synthetic_data['is_fraudulent'].sum()\n",
        "    total_count = len(synthetic_data)\n",
        "    print(f\"\\nFraud Statistics:\")\n",
        "    print(f\"Total records: {total_count}\")\n",
        "    print(f\"Fraudulent records: {fraud_count} ({(fraud_count/total_count)*100:.2f}%)\")\n",
        "\n",
        "    print(\"\\nSample of data with fraud indicators:\")\n",
        "    print(synthetic_data[synthetic_data['is_fraudulent'] == 1].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKXSLmZQTqQo",
        "outputId": "7f99a3bb-c55e-4017-c220-811aae1ad8f8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sdv/single_table/base.py:119: FutureWarning: The 'SingleTableMetadata' is deprecated. Please use the new 'Metadata' class for synthesizers.\n",
            "  warnings.warn(DEPRECATION_MSG, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sdv/single_table/base.py:104: UserWarning: We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic data generated and saved to 'synthetic_tax_data.csv'\n",
            "Training data saved to 'train_data.csv' (shape: (800, 8))\n",
            "Test data saved to 'test_data.csv' (shape: (200, 8))\n",
            "\n",
            "Fraud Statistics:\n",
            "Total records: 1000\n",
            "Fraudulent records: 50 (5.00%)\n",
            "\n",
            "Sample of data with fraud indicators:\n",
            "            income           filing_status state  compliance_score  \\\n",
            "48    33691.690000                  Single    RS         40.209392   \n",
            "66     5709.786694  Married Filing Jointly    PB         64.301654   \n",
            "71    59549.592785                  Single    ES         36.040054   \n",
            "95    24784.150000       Head of Household    MA         62.987728   \n",
            "115  222909.490915       Head of Household    PE         27.700184   \n",
            "\n",
            "        deductions   tax_paid  refund_claimed  is_fraudulent  \n",
            "48    23991.530799    6776.44          446.70              1  \n",
            "66     1186.310000     743.97           71.50              1  \n",
            "71    40178.626483   24160.02         1759.62              1  \n",
            "95    21211.948189    2309.08         6027.43              1  \n",
            "115  139820.364364  121046.02         4903.73              1  \n"
          ]
        }
      ]
    }
  ]
}